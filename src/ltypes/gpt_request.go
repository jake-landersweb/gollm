package ltypes

type GPTCompletionRequest struct {
	// A list of messages comprising the conversation so far.
	Messages []*GPTCompletionMessage `json:"messages"`

	// ID of the model to use.
	Model string `json:"model"`

	// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty int `json:"frequencypenalty,omitempty"`

	// Modify the likelihood of specified tokens appearing in the completion.
	// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	LogitBias map[int]int `json:"logit_bias,omitempty"`

	// The maximum number of tokens that can be generated in the chat completion.
	// The total length of input tokens and generated tokens is limited by the model's context length.
	MaxTokens uint `json:"max_tokens,omitempty"`

	// How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
	N uint `json:"n,omitempty"`

	// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	PresencePenalty int `json:"presence_penalty,omitempty"`

	// An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.
	// Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.
	ResponseFormat GPTRespFormat `json:"response_format,omitempty"`

	// This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
	Seed int `json:"seed,omitempty"`

	// Up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"`

	// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
	Stream bool `json:"stream,omitempty"`

	// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
	Temperature float64 `json:"temperature,omitempty"`

	// A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.
	Tools []*GPTTool `json:"tools,omitempty"`

	// Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function
	ToolChoice string `json:"tool_choice,omitempty"`

	// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	User string `json:"user,omitempty"`
}

type GPTRespFormat struct {
	Type string `json:"type"`
}

type GPTTool struct {
	// The type of the tool. Currently, only function is supported.
	Type string `json:"type"`

	// Function object for this tool
	Function GPTToolFunction `json:"function"`
}

type GPTToolFunction struct {
	// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
	Name string `json:"name"`

	// A description of what the function does, used by the model to choose when and how to call the function.
	Description string `json:"description,omitempty"`

	// The parameters the functions accepts, described as a JSON Schema object. See the guide for examples, and the JSON Schema reference for documentation about the format.
	Parameters map[string]string `json:"parameters,omitempty"`
}
